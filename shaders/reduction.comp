#version 430 core
#extension GL_NV_shader_atomic_float : require
#extension GL_ARB_compute_shader : require
#extension GL_ARB_shader_storage_buffer_object : require

#define SIZE 256
#define CLUSTERS 5
#define MAX_ITER 20

layout(local_size_x = 16, local_size_y = 16, local_size_z = 1) in;

layout(binding = 0) uniform sampler2D normals;
uniform ivec2 u_texSize;
uniform int u_iteration;

struct Cluster {
	vec3 cntr;
	uint size;
};

coherent restrict layout(std430, binding = 0) buffer destBuffer {
	Cluster clusters[CLUSTERS];
	bool moving;
};

coherent restrict layout(std430, binding = 1) buffer idxBuffer {
	uint indices[];
};

shared vec3 cache[CLUSTERS][SIZE];
shared uint sizeCache[CLUSTERS][SIZE];

void main() {

	const ivec2 pos = ivec2(gl_GlobalInvocationID.xy);
	const vec2 coords = vec2(pos.x / float(u_texSize.x), pos.y / float(u_texSize.y));
	const vec3 N = texture2D(normals, coords).rgb;
	const bool isNormal = length(N) != 0;

	const uint id = pos.y * (gl_WorkGroupSize.x + gl_NumWorkGroups.x) + pos.x;

	if(u_iteration == 1) { //initialize
		indices[id] = id % CLUSTERS;
	}

	uint centroidIDX = indices[id];
	
	if(id < CLUSTERS) {
        clusters[id].size = 0;
		clusters[id].cntr = vec3(0, 0, 0);
    }

    memoryBarrierShared();
    barrier();

	for(int i = 0; i < 5; ++i) {
		sizeCache[i][gl_LocalInvocationIndex] = 0;
		cache[i][gl_LocalInvocationIndex] = vec3(0, 0, 0);
	}
    sizeCache[centroidIDX][gl_LocalInvocationIndex] = int(isNormal);
	cache[centroidIDX][gl_LocalInvocationIndex] = int(isNormal) * N;
    
	int stepv = (SIZE >> 1); 
    while(stepv > 0) { //reduction over data in each working group
        if (gl_LocalInvocationIndex < stepv) {
			for(int i = 0; i < 5; ++i) {
				sizeCache[i][gl_LocalInvocationIndex] += sizeCache[i][gl_LocalInvocationIndex + stepv];
				cache[i][gl_LocalInvocationIndex] += cache[i][gl_LocalInvocationIndex + stepv];
			}
        }
        memoryBarrierShared();
        barrier();
        stepv = (stepv >> 1);
    }
    if (gl_LocalInvocationIndex == 0) {
		for(int i = 0; i < 5; ++i) {	//TODO
			if(sizeCache[i][0] != 0)
				atomicAdd(clusters[i].size, sizeCache[i][0]);
			if(cache[i][0].x != 0)
				atomicAdd(clusters[i].cntr.x, cache[i][0].x);
			if(cache[i][0].y != 0)
				atomicAdd(clusters[i].cntr.y, cache[i][0].y);
			if(cache[i][0].z != 0)
				atomicAdd(clusters[i].cntr.z, cache[i][0].z);
		}
    }

    memoryBarrier();
    barrier();
}